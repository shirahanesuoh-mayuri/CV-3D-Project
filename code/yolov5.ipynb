{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b242c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Essential Libraries\n",
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30303b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c80ca38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader():\n",
    "    def __init__(self, mode, transform, bc_df):\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_images, test_images = train_test_split(bc_df, test_size=0.3, random_state=42)\n",
    "        train_set, val_set = train_test_split(bc_df, test_size=0.2, random_state=42)\n",
    "        self.train_img, self.train_label = train_set[\"filepaths\"].values, train_set[\"labels\"].values\n",
    "        self.test_img, self.test_label = test_images[\"filepaths\"].values, test_images[\"labels\"].values\n",
    "        self.val_img, self.val_label = val_set[\"filepaths\"].values, val_set[\"labels\"].values\n",
    "        if mode == \"train\":\n",
    "            self.img, self.label =  self.train_img, self.train_label\n",
    "        elif mode == \"test\":\n",
    "            self.img, self.label =  self.test_img, self.test_label\n",
    "        elif mode == \"val\":\n",
    "            self.img, self.label =  self.val_img, self.val_label\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        img_path, label = self.img[i], self.label[i]\n",
    "        img = Image.open(img_path)\n",
    "        img = self.transform(img)\n",
    "        if \"NO\" in label:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        \n",
    "        \n",
    "        return img, torch.tensor(label)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff409b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "Drowsy        22348\n",
      "Non_Drowsy    19445\n",
      "Name: count, dtype: int64\n",
      "(41793, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepaths</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9790</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Drowsy...</td>\n",
       "      <td>Drowsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32366</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Non Dr...</td>\n",
       "      <td>Non_Drowsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7732</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Drowsy...</td>\n",
       "      <td>Drowsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15861</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Drowsy...</td>\n",
       "      <td>Drowsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Drowsy...</td>\n",
       "      <td>Drowsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36755</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Non Dr...</td>\n",
       "      <td>Non_Drowsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36939</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Non Dr...</td>\n",
       "      <td>Non_Drowsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36108</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Non Dr...</td>\n",
       "      <td>Non_Drowsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17470</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Drowsy...</td>\n",
       "      <td>Drowsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26208</th>\n",
       "      <td>dataset/Driver Drowsiness Dataset (DDD)/Non Dr...</td>\n",
       "      <td>Non_Drowsy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filepaths      labels\n",
       "9790   dataset/Driver Drowsiness Dataset (DDD)/Drowsy...      Drowsy\n",
       "32366  dataset/Driver Drowsiness Dataset (DDD)/Non Dr...  Non_Drowsy\n",
       "7732   dataset/Driver Drowsiness Dataset (DDD)/Drowsy...      Drowsy\n",
       "15861  dataset/Driver Drowsiness Dataset (DDD)/Drowsy...      Drowsy\n",
       "1836   dataset/Driver Drowsiness Dataset (DDD)/Drowsy...      Drowsy\n",
       "36755  dataset/Driver Drowsiness Dataset (DDD)/Non Dr...  Non_Drowsy\n",
       "36939  dataset/Driver Drowsiness Dataset (DDD)/Non Dr...  Non_Drowsy\n",
       "36108  dataset/Driver Drowsiness Dataset (DDD)/Non Dr...  Non_Drowsy\n",
       "17470  dataset/Driver Drowsiness Dataset (DDD)/Drowsy...      Drowsy\n",
       "26208  dataset/Driver Drowsiness Dataset (DDD)/Non Dr...  Non_Drowsy"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Drowsy='dataset/Driver Drowsiness Dataset (DDD)/Drowsy'\n",
    "Non_Drowsy=\"dataset/Driver Drowsiness Dataset (DDD)/Non Drowsy\"\n",
    "filepaths = []\n",
    "labels = []\n",
    "dict_list = [Drowsy,Non_Drowsy]\n",
    "for i, j in enumerate(dict_list):\n",
    "    flist = os.listdir(j)\n",
    "    for f in flist:\n",
    "        fpath = os.path.join(j, f)\n",
    "        if f[0] != \".\":\n",
    "            filepaths.append(fpath)\n",
    "            if i == 0:\n",
    "                labels.append('Drowsy')\n",
    "            elif i == 1:\n",
    "                labels.append('Non_Drowsy')\n",
    "Fseries = pd.Series(filepaths, name=\"filepaths\")\n",
    "Lseries = pd.Series(labels, name=\"labels\")\n",
    "bc_data = pd.concat([Fseries, Lseries], axis=1)\n",
    "bc_df = pd.DataFrame(bc_data)\n",
    "\n",
    "\n",
    "print(bc_df[\"labels\"].value_counts())\n",
    "\n",
    "\n",
    "print(bc_df.shape)\n",
    "bc_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdbe568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((198, 198)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "batch_size = 32\n",
    "train_dataset = dataloader(\"train\", transform, bc_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = dataloader(\"test\", transform, bc_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataset = dataloader(\"val\", transform, bc_df)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca48487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: gitpython>=3.1.30 in /opt/conda/lib/python3.8/site-packages (3.1.43)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from gitpython>=3.1.30) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30) (5.0.1)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 1.8s, installed 1 package: ['gitpython>=3.1.30']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2024-4-6 Python-3.9.19 torch-2.2.2+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 166 layers, 12947192 parameters, 0 gradients, 31.7 GFLOPs\n",
      "WARNING ‚ö†Ô∏è YOLOv5 ClassificationModel is not yet AutoShape compatible. You must pass torch tensors in BCHW to this model, i.e. shape(1,3,224,224).\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"./yolov5m-cls.pt\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0aa8982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1) \n",
    "criterion = nn.CrossEntropyLoss() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "589ba8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1045it [11:58,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Loss: 9607.8866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1045it [10:59,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Loss: 9607.8383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "519it [05:03,  1.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[1;32m      3\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader)):\n\u001b[1;32m      5\u001b[0m         labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m      6\u001b[0m         img, label \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/yolov5/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/yolov5/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/yolov5/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/yolov5/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/yolov5/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[30], line 30\u001b[0m, in \u001b[0;36mdataloader.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     28\u001b[0m img_path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel[i]\n\u001b[1;32m     29\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[0;32m---> 30\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m label:\n\u001b[1;32m     32\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/yolov5/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/envs/yolov5/lib/python3.9/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/yolov5/lib/python3.9/site-packages/torchvision/transforms/functional.py:175\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    173\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch =50\n",
    "for i in range(epoch):\n",
    "    losses = 0\n",
    "    for i, data in tqdm(enumerate(train_loader)):\n",
    "        labels = torch.zeros((data[1].shape[0], 1000))\n",
    "        img, label = data[0], data[1]\n",
    "        img = img.to(\"cuda:0\")\n",
    "        for i in range(data[1].shape[0]):\n",
    "            index = data[1][i].item()\n",
    "            labels[i][index] = 1\n",
    "        label = labels.to(\"cuda:0\")\n",
    "        label.requires_grad=True\n",
    "        result = model(img)\n",
    "        loss = criterion(result, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffb38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(train_loader.dataset), train_loader.dataset.train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c66f83b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "262it [04:37,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 / 8359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "acc = 0\n",
    "for i, data in tqdm(enumerate(val_loader)):\n",
    "    img, label = data[0], data[1]\n",
    "    img = img.to(\"cuda:0\")\n",
    "    result = model(img)\n",
    "    for num in range(len(img)):\n",
    "        if result[num][0].item() < result[num][1].item():\n",
    "            output = 1\n",
    "        else:\n",
    "            output = 0\n",
    "    \n",
    "        if output == label[num].item():\n",
    "            acc += 1\n",
    "print(f\"{acc} / {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd1f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
